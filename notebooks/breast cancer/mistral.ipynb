{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:07:34.471816Z",
     "iopub.status.busy": "2024-09-09T18:07:34.471356Z",
     "iopub.status.idle": "2024-09-09T18:07:36.859004Z",
     "shell.execute_reply": "2024-09-09T18:07:36.857236Z",
     "shell.execute_reply.started": "2024-09-09T18:07:34.471767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load necessar libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:09:07.231942Z",
     "iopub.status.busy": "2024-09-09T18:09:07.231379Z",
     "iopub.status.idle": "2024-09-09T18:09:07.291441Z",
     "shell.execute_reply": "2024-09-09T18:09:07.290191Z",
     "shell.execute_reply.started": "2024-09-09T18:09:07.231894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Loading the heart_disease_uci dataset\n",
    "train_data = pd.read_csv('../../data/breast_cancer/breast_cancer_data_train.csv')\n",
    "test_data = pd.read_csv('../../data/breast_cancer/breast_cancer_data_test.csv')\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:09:11.101110Z",
     "iopub.status.busy": "2024-09-09T18:09:11.100663Z",
     "iopub.status.idle": "2024-09-09T18:09:11.127511Z",
     "shell.execute_reply": "2024-09-09T18:09:11.126234Z",
     "shell.execute_reply.started": "2024-09-09T18:09:11.101068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Splitting the data into features and target\n",
    "X_train = train_data.iloc[:,:-1]\n",
    "y_train = train_data.iloc[:,-1]\n",
    "\n",
    "X_test = test_data.iloc[:,:-1]\n",
    "y_test = test_data.iloc[:,-1]\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:09:13.662139Z",
     "iopub.status.busy": "2024-09-09T18:09:13.661675Z",
     "iopub.status.idle": "2024-09-09T18:09:13.697567Z",
     "shell.execute_reply": "2024-09-09T18:09:13.695940Z",
     "shell.execute_reply.started": "2024-09-09T18:09:13.662081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Converting the data into dataframes\n",
    "X_train = pd.DataFrame(X_train, columns = train_data.columns[:-1])\n",
    "X_test = pd.DataFrame(X_test, columns = test_data.columns[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:25:28.703666Z",
     "iopub.status.busy": "2024-09-09T18:25:28.703025Z",
     "iopub.status.idle": "2024-09-09T18:29:13.468396Z",
     "shell.execute_reply": "2024-09-09T18:29:13.466833Z",
     "shell.execute_reply.started": "2024-09-09T18:25:28.703614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the Mistral model and tokenizer from Hugging Face\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
   
    "\n",
    "# Ensure compatibility by using AutoTokenizer and AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, num_labels=2,token=token,device_map=\"auto\")\n",
    "\n",
    "# Ensure pad_token_id is set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the prompt for in-context learning\n",
    "# Create a new function for constructing prompts\n",
    "def create_prompt(X_train, y_train, test_row):\n",
    "    \"\"\"\n",
    "    This function generates a prompt for the model by providing examples\n",
    "    of input features and their corresponding labels from the training set, followed by\n",
    "    the test instance for which we want the model to predict the label.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "\n",
    "    # Add the test instance for prediction\n",
    "    test_features = \", \".join([f\"{col}={test_row[col]}\" for col in X_train.columns])\n",
    "    prompt += f\"Input: {test_features} -> Output: ? \\n\"  \n",
    "\n",
    "    # Add examples from the training set to the prompt\n",
    "    for i, train_row in X_train.iterrows():\n",
    "        input_features = \", \".join([f\"{col}={train_row[col]}\" for col in X_train.columns])\n",
    "        label = y_train.iloc[i]\n",
    "        prompt += f\"Input: {input_features} -> Output: {label}\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example: Create a prompt for the first test instance\n",
    "prompt = create_prompt(X_train, y_train, X_test.iloc[0])  # Correct usage of .iloc\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "# Set the seed for reproducibility\n",
    "set_seed(42)\n",
    "# Tokenize the prompt with truncation\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=300)\n",
    "# Generate prediction\n",
    "input_length = inputs['input_ids'].shape[1]\n",
    "print(input_length)\n",
    "#output = model.generate(**inputs, max_new_tokens=10)  # Control how many tokens are generated\n",
    "output = model.generate(**inputs,min_length=input_length, max_new_tokens= 400, do_sample=True, temperature=0.7, top_k=50, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "# Decode the generated prediction\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_prediction(generated_text, prompt):\n",
    "    \"\"\"\n",
    "    Extracts the predicted label from the generated text.\n",
    "    Assumes the generated output follows the pattern \"Input: <features> -> Output: <prediction>\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regular expression to find the value after \"output=\"\n",
    "        #match = re.search(r'(Output|class|target)\\s*[:=]\\s*([-+]?\\d*\\.\\d+|\\d+)', generated_text)\n",
    "        match = re.search(r'Output\\s*:\\s*([-+]?\\d*\\.\\d+|\\d+)', generated_text)\n",
    "        if match:\n",
    "            prediction = match.group(1)\n",
    "            print(prediction)\n",
    "            return (int(round(float(prediction))) if prediction.replace('.', '', 1).isdigit() else None)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting prediction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "# Function to generate predictions for a batch of test data\n",
    "def in_context_learning(X_train, y_train, X_test, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates predictions for the test set using in-context learning.\n",
    "    \"\"\"\n",
    "    # Set the seed for reproducibility\n",
    "    # set_seed(42)\n",
    "    \n",
    "    \n",
    "    predictions = []\n",
    "    temp = []\n",
    "    for _, test_row in X_test.iterrows():\n",
    "        prompt = create_prompt(X_train, y_train, test_row)\n",
    "        \n",
    "        # Tokenize the prompt and generate output\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\",truncation=True, max_length=300)\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        output = model.generate(**inputs,min_length=input_length, max_new_tokens=200, do_sample=True, temperature=0.7, top_k=50,) # Adjust parameters as needed, these are working parameters\n",
    "        \n",
    "        # Decode the generated output\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        temp.append(generated_text)\n",
    "        # Extract the prediction from the generated text\n",
    "        predicted_label = extract_prediction(generated_text, prompt)\n",
    "        predictions.append(predicted_label)\n",
    "    \n",
    "    return predictions, temp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate predictions for the test set\n",
    "batch_size = 8  # Adjust batch size as needed\n",
    "predictions = []\n",
    "temp = []\n",
    "pd.DataFrame(predictions).to_csv('results/mistralmain.csv', index=False)\n",
    "pd.DataFrame(temp).to_csv('results/textmistral.csv', index=False)\n",
    "\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test[i:i + batch_size]\n",
    "    batch_predictions, textsp = in_context_learning(X_train, y_train, X_batch, model, tokenizer)\n",
    "    predictions.extend(batch_predictions)\n",
    "    temp.extend(textsp)\n",
    "    \n",
    "# Append the predictions to the results folder in mistral.csv file\n",
    "pd.DataFrame(predictions).to_csv('results/mistralmain.csv', mode='a', header=False, index=False)\n",
    "\n",
    "# Append the text to the results folder in textmistral.csv file\n",
    "pd.DataFrame(temp).to_csv('results/textmistral.csv', mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part of the code might throw an error if the predictions donot match the specified format in this case manual work to scrape the predictions must be performed from the generated csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Mistral model with confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "\n",
    "# Display confusion matrix\n",
    "print(confusion)\n",
    "\n",
    "# Calculate classification report\n",
    "report = classification_report(y_test, predictions)\n",
    "print(\"\\nClassification Report:\")\n",
    "# Display classification report\n",
    "print(report)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "# Display metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5670697,
     "sourceId": 9354327,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
