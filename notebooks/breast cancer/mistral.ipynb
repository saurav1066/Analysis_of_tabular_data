{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:07:34.471816Z",
     "iopub.status.busy": "2024-09-09T18:07:34.471356Z",
     "iopub.status.idle": "2024-09-09T18:07:36.859004Z",
     "shell.execute_reply": "2024-09-09T18:07:36.857236Z",
     "shell.execute_reply.started": "2024-09-09T18:07:34.471767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load necessar libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:09:07.231942Z",
     "iopub.status.busy": "2024-09-09T18:09:07.231379Z",
     "iopub.status.idle": "2024-09-09T18:09:07.291441Z",
     "shell.execute_reply": "2024-09-09T18:09:07.290191Z",
     "shell.execute_reply.started": "2024-09-09T18:09:07.231894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>859471</td>\n",
       "      <td>9.029</td>\n",
       "      <td>17.33</td>\n",
       "      <td>58.79</td>\n",
       "      <td>250.5</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.31300</td>\n",
       "      <td>0.04375</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>22.65</td>\n",
       "      <td>65.50</td>\n",
       "      <td>324.7</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.43650</td>\n",
       "      <td>1.25200</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>0.11750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>873593</td>\n",
       "      <td>21.090</td>\n",
       "      <td>26.57</td>\n",
       "      <td>142.70</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.24870</td>\n",
       "      <td>0.14960</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>...</td>\n",
       "      <td>33.48</td>\n",
       "      <td>176.50</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>0.14910</td>\n",
       "      <td>0.75840</td>\n",
       "      <td>0.67800</td>\n",
       "      <td>0.29030</td>\n",
       "      <td>0.4098</td>\n",
       "      <td>0.12840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>859196</td>\n",
       "      <td>9.173</td>\n",
       "      <td>13.86</td>\n",
       "      <td>59.20</td>\n",
       "      <td>260.9</td>\n",
       "      <td>0.07721</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.05988</td>\n",
       "      <td>0.02180</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>...</td>\n",
       "      <td>19.23</td>\n",
       "      <td>65.59</td>\n",
       "      <td>310.1</td>\n",
       "      <td>0.09836</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.13970</td>\n",
       "      <td>0.05087</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>0.08490</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88466802</td>\n",
       "      <td>10.650</td>\n",
       "      <td>25.22</td>\n",
       "      <td>68.01</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.09657</td>\n",
       "      <td>0.07234</td>\n",
       "      <td>0.02379</td>\n",
       "      <td>0.01615</td>\n",
       "      <td>0.1897</td>\n",
       "      <td>...</td>\n",
       "      <td>35.19</td>\n",
       "      <td>77.98</td>\n",
       "      <td>455.7</td>\n",
       "      <td>0.14990</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.06136</td>\n",
       "      <td>0.3409</td>\n",
       "      <td>0.08147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858970</td>\n",
       "      <td>10.170</td>\n",
       "      <td>14.88</td>\n",
       "      <td>64.55</td>\n",
       "      <td>311.9</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>0.08061</td>\n",
       "      <td>0.01084</td>\n",
       "      <td>0.01290</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>...</td>\n",
       "      <td>17.45</td>\n",
       "      <td>69.86</td>\n",
       "      <td>368.6</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.09866</td>\n",
       "      <td>0.02168</td>\n",
       "      <td>0.02579</td>\n",
       "      <td>0.3557</td>\n",
       "      <td>0.08020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    859471        9.029         17.33           58.79      250.5   \n",
       "1    873593       21.090         26.57          142.70     1311.0   \n",
       "2    859196        9.173         13.86           59.20      260.9   \n",
       "3  88466802       10.650         25.22           68.01      347.0   \n",
       "4    858970       10.170         14.88           64.55      311.9   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.10660           0.14130         0.31300              0.04375   \n",
       "1          0.11410           0.28320         0.24870              0.14960   \n",
       "2          0.07721           0.08751         0.05988              0.02180   \n",
       "3          0.09657           0.07234         0.02379              0.01615   \n",
       "4          0.11340           0.08061         0.01084              0.01290   \n",
       "\n",
       "   symmetry_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n",
       "0         0.2111  ...          22.65            65.50       324.7   \n",
       "1         0.2395  ...          33.48           176.50      2089.0   \n",
       "2         0.2341  ...          19.23            65.59       310.1   \n",
       "3         0.1897  ...          35.19            77.98       455.7   \n",
       "4         0.2743  ...          17.45            69.86       368.6   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0           0.14820            0.43650          1.25200               0.17500   \n",
       "1           0.14910            0.75840          0.67800               0.29030   \n",
       "2           0.09836            0.16780          0.13970               0.05087   \n",
       "3           0.14990            0.13980          0.11250               0.06136   \n",
       "4           0.12750            0.09866          0.02168               0.02579   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  diagnosis  \n",
       "0          0.4228                  0.11750          0  \n",
       "1          0.4098                  0.12840          1  \n",
       "2          0.3282                  0.08490          0  \n",
       "3          0.3409                  0.08147          0  \n",
       "4          0.3557                  0.08020          0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the heart_disease_uci dataset\n",
    "train_data = pd.read_csv('../../data/breast_cancer/breast_cancer_data_train.csv')\n",
    "test_data = pd.read_csv('../../data/breast_cancer/breast_cancer_data_test.csv')\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:09:11.101110Z",
     "iopub.status.busy": "2024-09-09T18:09:11.100663Z",
     "iopub.status.idle": "2024-09-09T18:09:11.127511Z",
     "shell.execute_reply": "2024-09-09T18:09:11.126234Z",
     "shell.execute_reply.started": "2024-09-09T18:09:11.101068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>859471</td>\n",
       "      <td>9.029</td>\n",
       "      <td>17.33</td>\n",
       "      <td>58.79</td>\n",
       "      <td>250.5</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.31300</td>\n",
       "      <td>0.04375</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>10.31</td>\n",
       "      <td>22.65</td>\n",
       "      <td>65.50</td>\n",
       "      <td>324.7</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.43650</td>\n",
       "      <td>1.25200</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>0.11750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>873593</td>\n",
       "      <td>21.090</td>\n",
       "      <td>26.57</td>\n",
       "      <td>142.70</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.24870</td>\n",
       "      <td>0.14960</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>...</td>\n",
       "      <td>26.68</td>\n",
       "      <td>33.48</td>\n",
       "      <td>176.50</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>0.14910</td>\n",
       "      <td>0.75840</td>\n",
       "      <td>0.67800</td>\n",
       "      <td>0.29030</td>\n",
       "      <td>0.4098</td>\n",
       "      <td>0.12840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>859196</td>\n",
       "      <td>9.173</td>\n",
       "      <td>13.86</td>\n",
       "      <td>59.20</td>\n",
       "      <td>260.9</td>\n",
       "      <td>0.07721</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.05988</td>\n",
       "      <td>0.02180</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>...</td>\n",
       "      <td>10.01</td>\n",
       "      <td>19.23</td>\n",
       "      <td>65.59</td>\n",
       "      <td>310.1</td>\n",
       "      <td>0.09836</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.13970</td>\n",
       "      <td>0.05087</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>0.08490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88466802</td>\n",
       "      <td>10.650</td>\n",
       "      <td>25.22</td>\n",
       "      <td>68.01</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.09657</td>\n",
       "      <td>0.07234</td>\n",
       "      <td>0.02379</td>\n",
       "      <td>0.01615</td>\n",
       "      <td>0.1897</td>\n",
       "      <td>...</td>\n",
       "      <td>12.25</td>\n",
       "      <td>35.19</td>\n",
       "      <td>77.98</td>\n",
       "      <td>455.7</td>\n",
       "      <td>0.14990</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.06136</td>\n",
       "      <td>0.3409</td>\n",
       "      <td>0.08147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858970</td>\n",
       "      <td>10.170</td>\n",
       "      <td>14.88</td>\n",
       "      <td>64.55</td>\n",
       "      <td>311.9</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>0.08061</td>\n",
       "      <td>0.01084</td>\n",
       "      <td>0.01290</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>...</td>\n",
       "      <td>11.02</td>\n",
       "      <td>17.45</td>\n",
       "      <td>69.86</td>\n",
       "      <td>368.6</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.09866</td>\n",
       "      <td>0.02168</td>\n",
       "      <td>0.02579</td>\n",
       "      <td>0.3557</td>\n",
       "      <td>0.08020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    859471        9.029         17.33           58.79      250.5   \n",
       "1    873593       21.090         26.57          142.70     1311.0   \n",
       "2    859196        9.173         13.86           59.20      260.9   \n",
       "3  88466802       10.650         25.22           68.01      347.0   \n",
       "4    858970       10.170         14.88           64.55      311.9   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.10660           0.14130         0.31300              0.04375   \n",
       "1          0.11410           0.28320         0.24870              0.14960   \n",
       "2          0.07721           0.08751         0.05988              0.02180   \n",
       "3          0.09657           0.07234         0.02379              0.01615   \n",
       "4          0.11340           0.08061         0.01084              0.01290   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2111  ...         10.31          22.65            65.50   \n",
       "1         0.2395  ...         26.68          33.48           176.50   \n",
       "2         0.2341  ...         10.01          19.23            65.59   \n",
       "3         0.1897  ...         12.25          35.19            77.98   \n",
       "4         0.2743  ...         11.02          17.45            69.86   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0       324.7           0.14820            0.43650          1.25200   \n",
       "1      2089.0           0.14910            0.75840          0.67800   \n",
       "2       310.1           0.09836            0.16780          0.13970   \n",
       "3       455.7           0.14990            0.13980          0.11250   \n",
       "4       368.6           0.12750            0.09866          0.02168   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0               0.17500          0.4228                  0.11750  \n",
       "1               0.29030          0.4098                  0.12840  \n",
       "2               0.05087          0.3282                  0.08490  \n",
       "3               0.06136          0.3409                  0.08147  \n",
       "4               0.02579          0.3557                  0.08020  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Splitting the data into features and target\n",
    "X_train = train_data.iloc[:,:-1]\n",
    "y_train = train_data.iloc[:,-1]\n",
    "\n",
    "X_test = test_data.iloc[:,:-1]\n",
    "y_test = test_data.iloc[:,-1]\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:09:13.662139Z",
     "iopub.status.busy": "2024-09-09T18:09:13.661675Z",
     "iopub.status.idle": "2024-09-09T18:09:13.697567Z",
     "shell.execute_reply": "2024-09-09T18:09:13.695940Z",
     "shell.execute_reply.started": "2024-09-09T18:09:13.662081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Converting the data into dataframes\n",
    "X_train = pd.DataFrame(X_train, columns = train_data.columns[:-1])\n",
    "X_test = pd.DataFrame(X_test, columns = test_data.columns[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:25:28.703666Z",
     "iopub.status.busy": "2024-09-09T18:25:28.703025Z",
     "iopub.status.idle": "2024-09-09T18:29:13.468396Z",
     "shell.execute_reply": "2024-09-09T18:29:13.466833Z",
     "shell.execute_reply.started": "2024-09-09T18:25:28.703614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e79f84c3d9940d6832fabfe57330fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Load the Mistral model and tokenizer from Hugging Face\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Ensure compatibility by using AutoTokenizer and AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, num_labels=2,token=token,device_map=\"auto\")\n",
    "\n",
    "# Ensure pad_token_id is set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the prompt for in-context learning\n",
    "# Create a new function for constructing prompts\n",
    "def create_prompt(X_train, y_train, test_row):\n",
    "    \"\"\"\n",
    "    This function generates a prompt for the model by providing examples\n",
    "    of input features and their corresponding labels from the training set, followed by\n",
    "    the test instance for which we want the model to predict the label.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "\n",
    "    # Add the test instance for prediction\n",
    "    test_features = \", \".join([f\"{col}={test_row[col]}\" for col in X_train.columns])\n",
    "    prompt += f\"Input: {test_features} -> Output: ? \\n\"  \n",
    "\n",
    "    # Add examples from the training set to the prompt\n",
    "    for i, train_row in X_train.iterrows():\n",
    "        input_features = \", \".join([f\"{col}={train_row[col]}\" for col in X_train.columns])\n",
    "        label = y_train.iloc[i]\n",
    "        prompt += f\"Input: {input_features} -> Output: {label}\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example: Create a prompt for the first test instance\n",
    "prompt = create_prompt(X_train, y_train, X_test.iloc[0])  # Correct usage of .iloc\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "# Set the seed for reproducibility\n",
    "set_seed(42)\n",
    "# Tokenize the prompt with truncation\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=300)\n",
    "# Generate prediction\n",
    "input_length = inputs['input_ids'].shape[1]\n",
    "print(input_length)\n",
    "#output = model.generate(**inputs, max_new_tokens=10)  # Control how many tokens are generated\n",
    "output = model.generate(**inputs,min_length=input_length, max_new_tokens= 400, do_sample=True, temperature=0.7, top_k=50, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "# Decode the generated prediction\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_prediction(generated_text, prompt):\n",
    "    \"\"\"\n",
    "    Extracts the predicted label from the generated text.\n",
    "    Assumes the generated output follows the pattern \"Input: <features> -> Output: <prediction>\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regular expression to find the value after \"output=\"\n",
    "        #match = re.search(r'(Output|class|target)\\s*[:=]\\s*([-+]?\\d*\\.\\d+|\\d+)', generated_text)\n",
    "        match = re.search(r'Output\\s*:\\s*([-+]?\\d*\\.\\d+|\\d+)', generated_text)\n",
    "        if match:\n",
    "            prediction = match.group(1)\n",
    "            print(prediction)\n",
    "            return (int(round(float(prediction))) if prediction.replace('.', '', 1).isdigit() else None)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting prediction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "# Function to generate predictions for a batch of test data\n",
    "def in_context_learning(X_train, y_train, X_test, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates predictions for the test set using in-context learning.\n",
    "    \"\"\"\n",
    "    # Set the seed for reproducibility\n",
    "    # set_seed(42)\n",
    "    \n",
    "    \n",
    "    predictions = []\n",
    "    temp = []\n",
    "    for _, test_row in X_test.iterrows():\n",
    "        prompt = create_prompt(X_train, y_train, test_row)\n",
    "        \n",
    "        # Tokenize the prompt and generate output\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\",truncation=True, max_length=300)\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        output = model.generate(**inputs,min_length=input_length, max_new_tokens=200, do_sample=True, temperature=0.7, top_k=50,) # Adjust parameters as needed, these are working parameters\n",
    "        \n",
    "        # Decode the generated output\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        temp.append(generated_text)\n",
    "        # Extract the prediction from the generated text\n",
    "        predicted_label = extract_prediction(generated_text, prompt)\n",
    "        predictions.append(predicted_label)\n",
    "    \n",
    "    return predictions, temp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate predictions for the test set\n",
    "batch_size = 8  # Adjust batch size as needed\n",
    "predictions = []\n",
    "temp = []\n",
    "pd.DataFrame(predictions).to_csv('results/mistralmain.csv', index=False)\n",
    "pd.DataFrame(temp).to_csv('results/textmistral.csv', index=False)\n",
    "\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test[i:i + batch_size]\n",
    "    batch_predictions, textsp = in_context_learning(X_train, y_train, X_batch, model, tokenizer)\n",
    "    predictions.extend(batch_predictions)\n",
    "    temp.extend(textsp)\n",
    "    \n",
    "# Append the predictions to the results folder in mistral.csv file\n",
    "pd.DataFrame(predictions).to_csv('results/mistralmain.csv', mode='a', header=False, index=False)\n",
    "\n",
    "# Append the text to the results folder in textmistral.csv file\n",
    "pd.DataFrame(temp).to_csv('results/textmistral.csv', mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part of the code might throw an error if the predictions donot match the specified format in this case manual work to scrape the predictions must be performed from the generated csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Mistral model with confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "\n",
    "# Display confusion matrix\n",
    "print(confusion)\n",
    "\n",
    "# Calculate classification report\n",
    "report = classification_report(y_test, predictions)\n",
    "print(\"\\nClassification Report:\")\n",
    "# Display classification report\n",
    "print(report)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "# Display metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5670697,
     "sourceId": 9354327,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
