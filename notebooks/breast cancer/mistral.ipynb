{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:07:34.471816Z",
     "iopub.status.busy": "2024-09-09T18:07:34.471356Z",
     "iopub.status.idle": "2024-09-09T18:07:36.859004Z",
     "shell.execute_reply": "2024-09-09T18:07:36.857236Z",
     "shell.execute_reply.started": "2024-09-09T18:07:34.471767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load necessar libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:09:07.231942Z",
     "iopub.status.busy": "2024-09-09T18:09:07.231379Z",
     "iopub.status.idle": "2024-09-09T18:09:07.291441Z",
     "shell.execute_reply": "2024-09-09T18:09:07.290191Z",
     "shell.execute_reply.started": "2024-09-09T18:09:07.231894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Loading the heart_disease_uci dataset\n",
    "train_data = pd.read_csv('../../data/breast_cancer/breast_cancer_data_train.csv')\n",
    "test_data = pd.read_csv('../../data/breast_cancer/breast_cancer_data_test.csv')\n",
    "train_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:09:11.101110Z",
     "iopub.status.busy": "2024-09-09T18:09:11.100663Z",
     "iopub.status.idle": "2024-09-09T18:09:11.127511Z",
     "shell.execute_reply": "2024-09-09T18:09:11.126234Z",
     "shell.execute_reply.started": "2024-09-09T18:09:11.101068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Splitting the data into features and target\n",
    "X_train = train_data.iloc[:,:-1]\n",
    "y_train = train_data.iloc[:,-1]\n",
    "\n",
    "X_test = test_data.iloc[:,:-1]\n",
    "y_test = test_data.iloc[:,-1]\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:09:13.662139Z",
     "iopub.status.busy": "2024-09-09T18:09:13.661675Z",
     "iopub.status.idle": "2024-09-09T18:09:13.697567Z",
     "shell.execute_reply": "2024-09-09T18:09:13.695940Z",
     "shell.execute_reply.started": "2024-09-09T18:09:13.662081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#Converting the data into dataframes\n",
    "X_train = pd.DataFrame(X_train, columns = train_data.columns[:-1])\n",
    "X_test = pd.DataFrame(X_test, columns = test_data.columns[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:25:28.703666Z",
     "iopub.status.busy": "2024-09-09T18:25:28.703025Z",
     "iopub.status.idle": "2024-09-09T18:29:13.468396Z",
     "shell.execute_reply": "2024-09-09T18:29:13.466833Z",
     "shell.execute_reply.started": "2024-09-09T18:25:28.703614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the Mistral model and tokenizer from Hugging Face\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "token = \"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\"\n",
    "\n",
    "# Ensure compatibility by using AutoTokenizer and AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, num_labels=2,token=token,device_map=\"auto\")\n",
    "\n",
    "# Ensure pad_token_id is set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modify the model for sequence classification\n",
    "# class MistralForClassification(torch.nn.Module):\n",
    "#     def __init__(self, model):\n",
    "#         super(MistralForClassification, self).__init__()\n",
    "#         self.model = model\n",
    "#         # Add a linear layer for classification (2 classes for heart disease)\n",
    "#         self.classifier = torch.nn.Linear(model.config.hidden_size, 2)\n",
    "    \n",
    "#     def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "#         # Get outputs from the Mistral model\n",
    "#         outputs = self.model(input_ids, attention_mask=attention_mask, output_hidden_states=False)\n",
    "        \n",
    "#         # Take the hidden states (last layer) from the model's output\n",
    "#         logits = self.classifier(outputs.logits[:, -1, :])  # Classification on [CLS] token\n",
    "        \n",
    "#         # Calculate loss if labels are provided\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss_fct = torch.nn.CrossEntropyLoss()\n",
    "#             loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
    "        \n",
    "#         return loss, logits\n",
    "\n",
    "# # Initialize the model with the classification head\n",
    "# classification_model = MistralForClassification(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:29:40.084160Z",
     "iopub.status.busy": "2024-09-09T18:29:40.082043Z",
     "iopub.status.idle": "2024-09-09T18:29:40.104036Z",
     "shell.execute_reply": "2024-09-09T18:29:40.101105Z",
     "shell.execute_reply.started": "2024-09-09T18:29:40.084024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # Custom dataset class for PyTorch\n",
    "# class HeartDiseaseDataset(Dataset):\n",
    "#     def __init__(self, features, targets, tokenizer):\n",
    "#         self.features = features\n",
    "#         self.targets = targets\n",
    "#         self.tokenizer = tokenizer\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.targets)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         # Convert row of features to string and tokenize\n",
    "#         row_str = \" \".join(map(str, self.features[idx]))\n",
    "#         inputs = self.tokenizer(row_str, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "#         label = torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "#         return inputs[\"input_ids\"].squeeze(0), inputs[\"attention_mask\"].squeeze(0), label\n",
    "\n",
    "\n",
    "# # Prepare dataset and dataloaders\n",
    "# train_dataset = HeartDiseaseDataset(X_train, y_train, tokenizer)\n",
    "# test_dataset = HeartDiseaseDataset(X_test, y_test, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # defining a custom collate function to pad the sequences\n",
    "# from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     input_ids = [item[0] for item in batch]\n",
    "#     attention_masks = [item[1] for item in batch]\n",
    "#     labels = [item[2] for item in batch]\n",
    "\n",
    "#     input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "#     attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "#     labels = torch.tensor(labels)\n",
    "\n",
    "#     return input_ids_padded, attention_masks_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=2, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T18:29:59.369318Z",
     "iopub.status.busy": "2024-09-09T18:29:59.368450Z",
     "iopub.status.idle": "2024-09-09T18:30:01.909025Z",
     "shell.execute_reply": "2024-09-09T18:30:01.906212Z",
     "shell.execute_reply.started": "2024-09-09T18:29:59.369240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import AdamW\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Set up the optimizer\n",
    "# optimizer = AdamW(classification_model.parameters(), lr=5e-5)\n",
    "\n",
    "# # Training function with gradient accumulation\n",
    "# def train(model, train_loader, optimizer, epochs=3, accumulation_steps=4):\n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = 0\n",
    "#         optimizer.zero_grad()\n",
    "#         for i, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#             loss = outputs.loss\n",
    "#             total_loss += loss.item()\n",
    "#             loss.backward()\n",
    "\n",
    "#             if (i + 1) % accumulation_steps == 0:\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "\n",
    "#         avg_loss = total_loss / len(train_loader)\n",
    "#         print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# # Train the model with gradient accumulation\n",
    "# train(classification_model, train_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to get predictions\n",
    "# def get_predictions(model, data_loader):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     for batch in data_loader:\n",
    "#         input_ids, attention_mask, labels = batch\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         _, pred = torch.max(outputs.logits, dim=1)\n",
    "#         predictions.extend(pred.tolist())\n",
    "#     return predictions\n",
    "\n",
    "# # Get predictions on test set\n",
    "# test_predictions = get_predictions(classification_model, test_loader)\n",
    "\n",
    "# # save predictions to results folder names bert.csv\n",
    "# pd.DataFrame(test_predictions).to_csv('results/mistral.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, test_loader):\n",
    "#     model.eval()\n",
    "#     total_correct = 0\n",
    "#     total_examples = 0\n",
    "#     all_labels = []\n",
    "#     all_preds = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             input_ids, attention_mask, labels = batch\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#             all_preds.extend(predictions.cpu().numpy())\n",
    "#             total_correct += (predictions == labels).sum().item()\n",
    "#             total_examples += labels.size(0)\n",
    "#     accuracy = total_correct / total_examples\n",
    "#     precision = precision_score(all_labels, all_preds)\n",
    "#     recall = recall_score(all_labels, all_preds)\n",
    "#     f1 = f1_score(all_labels, all_preds)\n",
    "#     return accuracy, precision, recall, f1\n",
    "\n",
    "# # Evaluate the Mistral model\n",
    "# mistral_accuracy, mistral_precision, mistral_recall, mistral_f1 = evaluate(classifiication_model, test_loader)\n",
    "# print(f'Mistral Model - Accuracy: {mistral_accuracy:.4f}, Precision: {mistral_precision:.4f}, Recall: {mistral_recall:.4f}, F1-Score: {mistral_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the prompt for in-context learning\n",
    "# Create a new function for constructing prompts\n",
    "def create_prompt(X_train, y_train, test_row):\n",
    "    \"\"\"\n",
    "    This function generates a prompt for the model by providing examples\n",
    "    of input features and their corresponding labels from the training set, followed by\n",
    "    the test instance for which we want the model to predict the label.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "\n",
    "    # Add the test instance for prediction\n",
    "    test_features = \", \".join([f\"{col}={test_row[col]}\" for col in X_train.columns])\n",
    "    prompt += f\"Input: {test_features} -> Output: ? \\n\"  \n",
    "\n",
    "    # Add examples from the training set to the prompt\n",
    "    for i, train_row in X_train.iterrows():\n",
    "        input_features = \", \".join([f\"{col}={train_row[col]}\" for col in X_train.columns])\n",
    "        label = y_train.iloc[i]\n",
    "        prompt += f\"Input: {input_features} -> Output: {label}\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Example: Create a prompt for the first test instance\n",
    "prompt = create_prompt(X_train, y_train, X_test.iloc[0])  # Correct usage of .iloc\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "# Set the seed for reproducibility\n",
    "set_seed(42)\n",
    "# Tokenize the prompt with truncation\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=300)\n",
    "# Generate prediction\n",
    "input_length = inputs['input_ids'].shape[1]\n",
    "print(input_length)\n",
    "#output = model.generate(**inputs, max_new_tokens=10)  # Control how many tokens are generated\n",
    "output = model.generate(**inputs,min_length=input_length, max_new_tokens= 400, do_sample=True, temperature=0.7, top_k=50, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "# Decode the generated prediction\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match = re.search(r'Output\\s*:\\s*([-+]?\\d*\\.\\d+|\\d+)', generated_text)\n",
    "# prediction = None\n",
    "# if match:\n",
    "#     prediction = match.group(1)\n",
    "#     print(prediction)\n",
    "#     print(type(prediction))\n",
    "# print(int(round(float(prediction))) if prediction.replace('.', '', 1).isdigit() else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the predicted label from the generated text\n",
    "# def extract_prediction(generated_text, prompt):\n",
    "#     \"\"\"\n",
    "#     Extracts the predicted label from the generated text.\n",
    "#     Assumes the generated output follows the pattern \"Input: <features> -> Output: <prediction>\".\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Extract the part of the output following \"Output: \" in the prompt\n",
    "#         prediction = generated_text.split(\"output=\")[-1].strip().split()[0]\n",
    "#         print(prediction)\n",
    "#         return int(prediction)  if prediction.isdigit() else None\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_prediction(generated_text, prompt):\n",
    "    \"\"\"\n",
    "    Extracts the predicted label from the generated text.\n",
    "    Assumes the generated output follows the pattern \"Input: <features> -> Output: <prediction>\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use regular expression to find the value after \"output=\"\n",
    "        #match = re.search(r'(Output|class|target)\\s*[:=]\\s*([-+]?\\d*\\.\\d+|\\d+)', generated_text)\n",
    "        match = re.search(r'Output\\s*:\\s*([-+]?\\d*\\.\\d+|\\d+)', generated_text)\n",
    "        if match:\n",
    "            prediction = match.group(1)\n",
    "            print(prediction)\n",
    "            return (int(round(float(prediction))) if prediction.replace('.', '', 1).isdigit() else None)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting prediction: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "# Function to generate predictions for a batch of test data\n",
    "def in_context_learning(X_train, y_train, X_test, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Generates predictions for the test set using in-context learning.\n",
    "    \"\"\"\n",
    "    # Set the seed for reproducibility\n",
    "    # set_seed(42)\n",
    "    \n",
    "    \n",
    "    predictions = []\n",
    "    temp = []\n",
    "    for _, test_row in X_test.iterrows():\n",
    "        prompt = create_prompt(X_train, y_train, test_row)\n",
    "        \n",
    "        # Tokenize the prompt and generate output\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\",truncation=True, max_length=300)\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        output = model.generate(**inputs,min_length=input_length, max_new_tokens=200, do_sample=True, temperature=0.7, top_k=50,) # Adjust parameters as needed, these are working parameters\n",
    "        \n",
    "        # Decode the generated output\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        temp.append(generated_text)\n",
    "        # Extract the prediction from the generated text\n",
    "        predicted_label = extract_prediction(generated_text, prompt)\n",
    "        predictions.append(predicted_label)\n",
    "    \n",
    "    return predictions, temp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate predictions for the test set\n",
    "batch_size = 8  # Adjust batch size as needed\n",
    "predictions = []\n",
    "temp = []\n",
    "pd.DataFrame(predictions).to_csv('results/mistralmain.csv', index=False)\n",
    "pd.DataFrame(temp).to_csv('results/textmistral.csv', index=False)\n",
    "\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    X_batch = X_test[i:i + batch_size]\n",
    "    batch_predictions, textsp = in_context_learning(X_train, y_train, X_batch, model, tokenizer)\n",
    "    predictions.extend(batch_predictions)\n",
    "    temp.extend(textsp)\n",
    "    \n",
    "# Append the predictions to the results folder in mistral.csv file\n",
    "pd.DataFrame(predictions).to_csv('results/mistralmain.csv', mode='a', header=False, index=False)\n",
    "\n",
    "# Append the text to the results folder in textmistral.csv file\n",
    "pd.DataFrame(temp).to_csv('results/textmistral.csv', mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Mistral model with confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion = confusion_matrix(y_test, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "\n",
    "# Display confusion matrix\n",
    "print(confusion)\n",
    "\n",
    "# Calculate classification report\n",
    "report = classification_report(y_test, predictions)\n",
    "print(\"\\nClassification Report:\")\n",
    "# Display classification report\n",
    "print(report)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "# Display metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5670697,
     "sourceId": 9354327,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
