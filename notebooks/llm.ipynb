{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9017905,"sourceType":"datasetVersion","datasetId":5434026},{"sourceId":9524338,"sourceType":"datasetVersion","datasetId":5799509}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# importing the required modules\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:59:50.746932Z","iopub.execute_input":"2024-10-01T13:59:50.747326Z","iopub.status.idle":"2024-10-01T13:59:58.311474Z","shell.execute_reply.started":"2024-10-01T13:59:50.747288Z","shell.execute_reply":"2024-10-01T13:59:58.310142Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Loading the heart_disease_uci dataset\ntrain_data = pd.read_csv('/kaggle/input/heart-llm/data_train.csv')\ntest_data = pd.read_csv('/kaggle/input/heart-llm/data_test.csv')\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:59:58.313279Z","iopub.execute_input":"2024-10-01T13:59:58.313860Z","iopub.status.idle":"2024-10-01T13:59:58.531908Z","shell.execute_reply.started":"2024-10-01T13:59:58.313805Z","shell.execute_reply":"2024-10-01T13:59:58.530825Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"    id  age  sex  dataset   cp  trestbps    chol  fbs  restecg  thalch  exang  \\\n0  881   62  1.0      3.0  0.0    146.12  170.00  0.0      2.0  120.00    1.0   \n1  458   54  1.0      1.0  2.0    150.00  216.98  0.0      1.0  122.00    0.0   \n2  798   51  1.0      3.0  2.0    134.86  339.00  0.0      1.0  132.41    1.0   \n3   26   50  0.0      0.0  2.0    120.00  219.00  0.0      1.0  158.00    0.0   \n4   85   52  1.0      0.0  1.0    120.00  325.00  0.0      1.0  172.00    0.0   \n\n   oldpeak  slope   ca  thal  num  \n0    3.000    0.0  0.0   2.0    1  \n1    0.000    2.0  0.0   2.0    0  \n2    2.943    1.0  0.0   2.0    1  \n3    1.600    1.0  0.0   1.0    0  \n4    0.200    2.0  0.0   1.0    0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>dataset</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalch</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>881</td>\n      <td>62</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>146.12</td>\n      <td>170.00</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>120.00</td>\n      <td>1.0</td>\n      <td>3.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>458</td>\n      <td>54</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>150.00</td>\n      <td>216.98</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>122.00</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>798</td>\n      <td>51</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>134.86</td>\n      <td>339.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>132.41</td>\n      <td>1.0</td>\n      <td>2.943</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26</td>\n      <td>50</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>120.00</td>\n      <td>219.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>158.00</td>\n      <td>0.0</td>\n      <td>1.600</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>85</td>\n      <td>52</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>120.00</td>\n      <td>325.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>172.00</td>\n      <td>0.0</td>\n      <td>0.200</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Splitting the data into features and target\nX_train = train_data.iloc[:,:-1]\ny_train = train_data.iloc[:,-1]\n\nX_test = test_data.iloc[:,:-1]\ny_test = test_data.iloc[:,-1]\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:59:58.533086Z","iopub.execute_input":"2024-10-01T13:59:58.533429Z","iopub.status.idle":"2024-10-01T13:59:58.562024Z","shell.execute_reply.started":"2024-10-01T13:59:58.533388Z","shell.execute_reply":"2024-10-01T13:59:58.560717Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"    id  age  sex  dataset   cp  trestbps    chol  fbs  restecg  thalch  exang  \\\n0  881   62  1.0      3.0  0.0    146.12  170.00  0.0      2.0  120.00    1.0   \n1  458   54  1.0      1.0  2.0    150.00  216.98  0.0      1.0  122.00    0.0   \n2  798   51  1.0      3.0  2.0    134.86  339.00  0.0      1.0  132.41    1.0   \n3   26   50  0.0      0.0  2.0    120.00  219.00  0.0      1.0  158.00    0.0   \n4   85   52  1.0      0.0  1.0    120.00  325.00  0.0      1.0  172.00    0.0   \n\n   oldpeak  slope   ca  thal  \n0    3.000    0.0  0.0   2.0  \n1    0.000    2.0  0.0   2.0  \n2    2.943    1.0  0.0   2.0  \n3    1.600    1.0  0.0   1.0  \n4    0.200    2.0  0.0   1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>dataset</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalch</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>881</td>\n      <td>62</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>0.0</td>\n      <td>146.12</td>\n      <td>170.00</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>120.00</td>\n      <td>1.0</td>\n      <td>3.000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>458</td>\n      <td>54</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>150.00</td>\n      <td>216.98</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>122.00</td>\n      <td>0.0</td>\n      <td>0.000</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>798</td>\n      <td>51</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>134.86</td>\n      <td>339.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>132.41</td>\n      <td>1.0</td>\n      <td>2.943</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26</td>\n      <td>50</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>120.00</td>\n      <td>219.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>158.00</td>\n      <td>0.0</td>\n      <td>1.600</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>85</td>\n      <td>52</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>120.00</td>\n      <td>325.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>172.00</td>\n      <td>0.0</td>\n      <td>0.200</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:59:58.564419Z","iopub.execute_input":"2024-10-01T13:59:58.565183Z","iopub.status.idle":"2024-10-01T13:59:58.583508Z","shell.execute_reply.started":"2024-10-01T13:59:58.565139Z","shell.execute_reply":"2024-10-01T13:59:58.581963Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Convert to pandas DataFrame for easier handling\ntrain_data = pd.DataFrame(X_train)\ntrain_data['target'] = y_train.values\n\ntest_data = pd.DataFrame(X_test)\ntest_data['target'] = y_test.values","metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:59:58.586298Z","iopub.execute_input":"2024-10-01T13:59:58.586734Z","iopub.status.idle":"2024-10-01T13:59:58.596582Z","shell.execute_reply.started":"2024-10-01T13:59:58.586692Z","shell.execute_reply":"2024-10-01T13:59:58.595026Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"The code initializes a BERT tokenizer using the `BertTokenizer` class from the `transformers` library. The tokenizer is set up with the pre-trained model `bert-base-uncased`, which converts all text to lowercase before tokenizing. A function named `tokenize_row` is defined to tokenize each row of a DataFrame individually. This function converts the row to a single string by joining all its values with spaces, as BERT expects textual input. The tokenizer processes the string, adding padding to the maximum length, truncating if necessary, and returning the result as PyTorch tensors. The training and testing data are then tokenized by iterating over each row in the `train_data` and `test_data` DataFrames, applying the `tokenize_row` function, and storing the tokenized inputs in the `train_inputs` and `test_inputs` lists, respectively.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Function to tokenize each row individually\ndef tokenize_row(row, tokenizer):\n    # Convert the row to string, as BERT expects textual input\n    row_str = \" \".join(map(str, row.values))\n    inputs = tokenizer(row_str, padding='max_length', truncation=True, return_tensors=\"pt\")\n    return inputs\n\n# Tokenize the training and testing data\ntrain_inputs = [tokenize_row(row, tokenizer) for _, row in train_data.iterrows()]\ntest_inputs = [tokenize_row(row, tokenizer) for _, row in test_data.iterrows()]","metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:59:58.598046Z","iopub.execute_input":"2024-10-01T13:59:58.598460Z","iopub.status.idle":"2024-10-01T14:00:03.440106Z","shell.execute_reply.started":"2024-10-01T13:59:58.598411Z","shell.execute_reply":"2024-10-01T14:00:03.438836Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a2527fc1f814857b7849ca4071c645a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c98ce764affd4fb7b01b18076743ae08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5b466dc99974a3f8f1f0f3f62891924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81fa0cf3d78349a6a1c32b6aa180bf4c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code imports necessary modules from the `torch` library and defines a custom dataset class `HeartDiseaseDataset` that inherits from `torch.utils.data.Dataset`. The class is initialized with `inputs` and `labels`, storing them as instance variables. The `__len__` method returns the number of samples in the dataset, while the `__getitem__` method retrieves the input IDs, attention mask, and label for a given index, removing the batch dimension from the input IDs and attention mask. The dataset and dataloader are then created for both training and testing data. `train_dataset` and `test_dataset` are instances of `HeartDiseaseDataset`, initialized with `train_inputs` and `y_train` for training, and `test_inputs` and `y_test` for testing. The `DataLoader` class is used to create `train_loader` and `test_loader` with a batch size of 16, shuffling the training data.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass HeartDiseaseDataset(Dataset):\n    def __init__(self, inputs, labels):\n        self.inputs = inputs\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        input_ids = self.inputs[idx]['input_ids'].squeeze(0)  # Remove batch dimension\n        attention_mask = self.inputs[idx]['attention_mask'].squeeze(0)\n        label = torch.tensor(self.labels.iloc[idx])\n        return input_ids, attention_mask, label\n\n# Create dataset and dataloader\ntrain_dataset = HeartDiseaseDataset(train_inputs, y_train)\ntest_dataset = HeartDiseaseDataset(test_inputs, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:00:03.441970Z","iopub.execute_input":"2024-10-01T14:00:03.442351Z","iopub.status.idle":"2024-10-01T14:00:03.451279Z","shell.execute_reply.started":"2024-10-01T14:00:03.442306Z","shell.execute_reply":"2024-10-01T14:00:03.450014Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The code imports `BertForSequenceClassification` and `AdamW` from the `transformers` library. It then loads a pre-trained BERT model for sequence classification with two labels using `BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)`. The optimizer is set up using `AdamW` with the model parameters and a learning rate of `5e-5`. A training function `train` is defined, which sets the model to training mode, initializes the total loss, and iterates over batches in the `train_loader`. For each batch, it zeroes the gradients, performs a forward pass to compute the loss, accumulates the loss, performs backpropagation, and updates the model parameters using the optimizer. The average loss for the epoch is returned. The training loop runs for three epochs, calling the `train` function for each epoch and printing the average loss.","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW\n\n# Load pre-trained BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Set up the optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training function\ndef train(model, train_loader, optimizer):\n    model.train()\n    total_loss = 0\n    for batch in train_loader:\n        input_ids, attention_mask, labels = batch\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n    return total_loss / len(train_loader)\n\n\n# Training loop\nnum_epochs = 3\nfor epoch in range(num_epochs):\n    avg_loss = train(model, train_loader, optimizer)\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T14:00:03.453131Z","iopub.execute_input":"2024-10-01T14:00:03.454417Z","iopub.status.idle":"2024-10-01T15:36:08.472387Z","shell.execute_reply.started":"2024-10-01T14:00:03.454361Z","shell.execute_reply":"2024-10-01T15:36:08.469635Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"516a8c5603b24aceb977e4d61836c2e7"}},"metadata":{}},{"name":"stderr","text":"A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Loss: 0.6388\nEpoch 2/3, Loss: 0.6890\nEpoch 3/3, Loss: 0.7061\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code defines a function `get_predictions` to obtain predictions from a model using a data loader. The function sets the model to evaluation mode with `model.eval()` and initializes an empty list for predictions. It uses `torch.no_grad()` to disable gradient calculation, iterating over batches in the `data_loader`. For each batch, it extracts `input_ids` and `attention_mask`, performs a forward pass through the model, and computes the predicted class labels using `torch.max` on the model's logits. The predictions are converted to a NumPy array and appended to the predictions list. Finally, the function returns the list of predictions. The function is then called to get predictions on the test set, storing the results in `test_predictions`.","metadata":{}},{"cell_type":"code","source":"# Get Predictions\n\n# Function to get predictions\ndef get_predictions(model, data_loader):\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids, attention_mask, _ = batch\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            _, pred = torch.max(outputs.logits, dim=1)\n            predictions.extend(pred.cpu().numpy())\n    return predictions\n\n# Get predictions on test set\ntest_predictions = get_predictions(model, test_loader)\n\n# save predictions to results folder names bert.csv\npd.DataFrame(test_predictions).to_csv('/kaggle/working/bert.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:36:08.476999Z","iopub.execute_input":"2024-10-01T15:36:08.479294Z","iopub.status.idle":"2024-10-01T15:38:29.068350Z","shell.execute_reply.started":"2024-10-01T15:36:08.479218Z","shell.execute_reply":"2024-10-01T15:38:29.066954Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, test_loader):\n    model.eval()\n    total_correct = 0\n    total_examples = 0\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids, attention_mask, labels = batch\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            total_correct += (predictions == labels).sum().item()\n            total_examples += labels.size(0)\n    accuracy = total_correct / total_examples\n    return accuracy\n\n# Evaluate the model\naccuracy = evaluate(model, test_loader)\nprint(f'Test Accuracy: {accuracy:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:38:29.073181Z","iopub.execute_input":"2024-10-01T15:38:29.073774Z","iopub.status.idle":"2024-10-01T15:40:49.326100Z","shell.execute_reply.started":"2024-10-01T15:38:29.073717Z","shell.execute_reply":"2024-10-01T15:40:49.324924Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Test Accuracy: 0.5924\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code imports `confusion_matrix` from `sklearn.metrics` and `numpy` as `np`. It defines a function `evaluate_confusion_matrix` to create a confusion matrix for evaluating the model. The function sets the model to evaluation mode with `model.eval()` and initializes empty lists for predictions and actual labels. Using `torch.no_grad()` to disable gradient calculation, it iterates over batches in the `test_loader`. For each batch, it extracts `input_ids`, `attention_mask`, and `labels`, performs a forward pass through the model, and appends the predicted class labels (obtained using `torch.argmax` on the model's logits) and actual labels to their respective lists. The function returns a confusion matrix computed from the actual and predicted labels using `confusion_matrix(actuals, predictions)`. The confusion matrix is then evaluated and printed by calling `evaluate_confusion_matrix(model, test_loader)` and storing the result in `conf_matrix`.","metadata":{}},{"cell_type":"code","source":"# create a confusion matrix to evaluate the model\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\ndef evaluate_confusion_matrix(model, test_loader):\n    model.eval()\n    predictions = []\n    actuals = []\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids, attention_mask, labels = batch\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            predictions.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy())\n            actuals.extend(labels.cpu().numpy())\n    return confusion_matrix(actuals, predictions)\n\nconf_matrix = evaluate_confusion_matrix(model, test_loader)\nprint(conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T15:40:49.327788Z","iopub.execute_input":"2024-10-01T15:40:49.328290Z","iopub.status.idle":"2024-10-01T15:43:11.077558Z","shell.execute_reply.started":"2024-10-01T15:40:49.328239Z","shell.execute_reply":"2024-10-01T15:43:11.076105Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[[  0  75]\n [  0 109]]\n","output_type":"stream"}]}]}