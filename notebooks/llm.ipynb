{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:59:50.747326Z","iopub.status.busy":"2024-10-01T13:59:50.746932Z","iopub.status.idle":"2024-10-01T13:59:58.311474Z","shell.execute_reply":"2024-10-01T13:59:58.310142Z","shell.execute_reply.started":"2024-10-01T13:59:50.747288Z"},"trusted":true},"outputs":[],"source":["# importing the required modules\n","\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:59:58.313860Z","iopub.status.busy":"2024-10-01T13:59:58.313279Z","iopub.status.idle":"2024-10-01T13:59:58.531908Z","shell.execute_reply":"2024-10-01T13:59:58.530825Z","shell.execute_reply.started":"2024-10-01T13:59:58.313805Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>dataset</th>\n","      <th>cp</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalch</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>slope</th>\n","      <th>ca</th>\n","      <th>thal</th>\n","      <th>num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>881</td>\n","      <td>62</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>146.12</td>\n","      <td>170.00</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>120.00</td>\n","      <td>1.0</td>\n","      <td>3.000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>458</td>\n","      <td>54</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>150.00</td>\n","      <td>216.98</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>122.00</td>\n","      <td>0.0</td>\n","      <td>0.000</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>798</td>\n","      <td>51</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>134.86</td>\n","      <td>339.00</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>132.41</td>\n","      <td>1.0</td>\n","      <td>2.943</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>26</td>\n","      <td>50</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>120.00</td>\n","      <td>219.00</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>158.00</td>\n","      <td>0.0</td>\n","      <td>1.600</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>85</td>\n","      <td>52</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>120.00</td>\n","      <td>325.00</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>172.00</td>\n","      <td>0.0</td>\n","      <td>0.200</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  age  sex  dataset   cp  trestbps    chol  fbs  restecg  thalch  exang  \\\n","0  881   62  1.0      3.0  0.0    146.12  170.00  0.0      2.0  120.00    1.0   \n","1  458   54  1.0      1.0  2.0    150.00  216.98  0.0      1.0  122.00    0.0   \n","2  798   51  1.0      3.0  2.0    134.86  339.00  0.0      1.0  132.41    1.0   \n","3   26   50  0.0      0.0  2.0    120.00  219.00  0.0      1.0  158.00    0.0   \n","4   85   52  1.0      0.0  1.0    120.00  325.00  0.0      1.0  172.00    0.0   \n","\n","   oldpeak  slope   ca  thal  num  \n","0    3.000    0.0  0.0   2.0    1  \n","1    0.000    2.0  0.0   2.0    0  \n","2    2.943    1.0  0.0   2.0    1  \n","3    1.600    1.0  0.0   1.0    0  \n","4    0.200    2.0  0.0   1.0    0  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["#Loading the heart_disease_uci dataset\n","train_data = pd.read_csv('..\\data\\heart_disease\\data_train.csv')\n","test_data = pd.read_csv('..\\data\\heart_disease\\data_test.csv')\n","train_data.head()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:59:58.533429Z","iopub.status.busy":"2024-10-01T13:59:58.533086Z","iopub.status.idle":"2024-10-01T13:59:58.562024Z","shell.execute_reply":"2024-10-01T13:59:58.560717Z","shell.execute_reply.started":"2024-10-01T13:59:58.533388Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>age</th>\n","      <th>sex</th>\n","      <th>dataset</th>\n","      <th>cp</th>\n","      <th>trestbps</th>\n","      <th>chol</th>\n","      <th>fbs</th>\n","      <th>restecg</th>\n","      <th>thalch</th>\n","      <th>exang</th>\n","      <th>oldpeak</th>\n","      <th>slope</th>\n","      <th>ca</th>\n","      <th>thal</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>881</td>\n","      <td>62</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>146.12</td>\n","      <td>170.00</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>120.00</td>\n","      <td>1.0</td>\n","      <td>3.000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>458</td>\n","      <td>54</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>150.00</td>\n","      <td>216.98</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>122.00</td>\n","      <td>0.0</td>\n","      <td>0.000</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>798</td>\n","      <td>51</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>134.86</td>\n","      <td>339.00</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>132.41</td>\n","      <td>1.0</td>\n","      <td>2.943</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>26</td>\n","      <td>50</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>120.00</td>\n","      <td>219.00</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>158.00</td>\n","      <td>0.0</td>\n","      <td>1.600</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>85</td>\n","      <td>52</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>120.00</td>\n","      <td>325.00</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>172.00</td>\n","      <td>0.0</td>\n","      <td>0.200</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  age  sex  dataset   cp  trestbps    chol  fbs  restecg  thalch  exang  \\\n","0  881   62  1.0      3.0  0.0    146.12  170.00  0.0      2.0  120.00    1.0   \n","1  458   54  1.0      1.0  2.0    150.00  216.98  0.0      1.0  122.00    0.0   \n","2  798   51  1.0      3.0  2.0    134.86  339.00  0.0      1.0  132.41    1.0   \n","3   26   50  0.0      0.0  2.0    120.00  219.00  0.0      1.0  158.00    0.0   \n","4   85   52  1.0      0.0  1.0    120.00  325.00  0.0      1.0  172.00    0.0   \n","\n","   oldpeak  slope   ca  thal  \n","0    3.000    0.0  0.0   2.0  \n","1    0.000    2.0  0.0   2.0  \n","2    2.943    1.0  0.0   2.0  \n","3    1.600    1.0  0.0   1.0  \n","4    0.200    2.0  0.0   1.0  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["#Splitting the data into features and target\n","X_train = train_data.iloc[:,:-1]\n","y_train = train_data.iloc[:,-1]\n","\n","X_test = test_data.iloc[:,:-1]\n","y_test = test_data.iloc[:,-1]\n","X_train.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:59:58.565183Z","iopub.status.busy":"2024-10-01T13:59:58.564419Z","iopub.status.idle":"2024-10-01T13:59:58.583508Z","shell.execute_reply":"2024-10-01T13:59:58.581963Z","shell.execute_reply.started":"2024-10-01T13:59:58.565139Z"},"trusted":true},"outputs":[],"source":["#Scaling the data\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:59:58.586734Z","iopub.status.busy":"2024-10-01T13:59:58.586298Z","iopub.status.idle":"2024-10-01T13:59:58.596582Z","shell.execute_reply":"2024-10-01T13:59:58.595026Z","shell.execute_reply.started":"2024-10-01T13:59:58.586692Z"},"trusted":true},"outputs":[],"source":["# Convert to pandas DataFrame for easier handling\n","train_data = pd.DataFrame(X_train)\n","train_data['target'] = y_train.values\n","\n","test_data = pd.DataFrame(X_test)\n","test_data['target'] = y_test.values"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["#getting dummies for the train and test data categorical columns\n","\n","categorical_columns = train_data.select_dtypes(include='object').columns\n","train_data = pd.get_dummies(train_data, columns=categorical_columns)\n","test_data = pd.get_dummies(test_data, columns=categorical_columns)\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["The code initializes a BERT tokenizer using the `BertTokenizer` class from the `transformers` library. The tokenizer is set up with the pre-trained model `bert-base-uncased`, which converts all text to lowercase before tokenizing. A function named `tokenize_row` is defined to tokenize each row of a DataFrame individually. This function converts the row to a single string by joining all its values with spaces, as BERT expects textual input. The tokenizer processes the string, adding padding to the maximum length, truncating if necessary, and returning the result as PyTorch tensors. The training and testing data are then tokenized by iterating over each row in the `train_data` and `test_data` DataFrames, applying the `tokenize_row` function, and storing the tokenized inputs in the `train_inputs` and `test_inputs` lists, respectively."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T13:59:58.598460Z","iopub.status.busy":"2024-10-01T13:59:58.598046Z","iopub.status.idle":"2024-10-01T14:00:03.440106Z","shell.execute_reply":"2024-10-01T14:00:03.438836Z","shell.execute_reply.started":"2024-10-01T13:59:58.598411Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["from transformers import BertTokenizer\n","\n","# Initialize the tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Function to tokenize each row individually\n","def tokenize_row(row, tokenizer):\n","    # Convert the row to string, as BERT expects textual input\n","    row_str = \" \".join(map(str, row.values))\n","    inputs = tokenizer(row_str, padding='max_length', truncation=True, return_tensors=\"pt\")\n","    return inputs\n","\n","# Tokenize the training and testing data\n","train_inputs = [tokenize_row(row, tokenizer) for _, row in train_data.iterrows()]\n","test_inputs = [tokenize_row(row, tokenizer) for _, row in test_data.iterrows()]"]},{"cell_type":"markdown","metadata":{},"source":["The code imports necessary modules from the `torch` library and defines a custom dataset class `HeartDiseaseDataset` that inherits from `torch.utils.data.Dataset`. The class is initialized with `inputs` and `labels`, storing them as instance variables. The `__len__` method returns the number of samples in the dataset, while the `__getitem__` method retrieves the input IDs, attention mask, and label for a given index, removing the batch dimension from the input IDs and attention mask. The dataset and dataloader are then created for both training and testing data. `train_dataset` and `test_dataset` are instances of `HeartDiseaseDataset`, initialized with `train_inputs` and `y_train` for training, and `test_inputs` and `y_test` for testing. The `DataLoader` class is used to create `train_loader` and `test_loader` with a batch size of 16, shuffling the training data."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T14:00:03.442351Z","iopub.status.busy":"2024-10-01T14:00:03.441970Z","iopub.status.idle":"2024-10-01T14:00:03.451279Z","shell.execute_reply":"2024-10-01T14:00:03.450014Z","shell.execute_reply.started":"2024-10-01T14:00:03.442306Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class HeartDiseaseDataset(Dataset):\n","    def __init__(self, inputs, labels):\n","        self.inputs = inputs\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        input_ids = self.inputs[idx]['input_ids'].squeeze(0)  # Remove batch dimension\n","        attention_mask = self.inputs[idx]['attention_mask'].squeeze(0)\n","        label = torch.tensor(self.labels.iloc[idx])\n","        return input_ids, attention_mask, label\n","\n","# Create dataset and dataloader\n","train_dataset = HeartDiseaseDataset(train_inputs, y_train)\n","test_dataset = HeartDiseaseDataset(test_inputs, y_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=16)\n"]},{"cell_type":"markdown","metadata":{},"source":["The code imports `BertForSequenceClassification` and `AdamW` from the `transformers` library. It then loads a pre-trained BERT model for sequence classification with two labels using `BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)`. The optimizer is set up using `AdamW` with the model parameters and a learning rate of `5e-5`. A training function `train` is defined, which sets the model to training mode, initializes the total loss, and iterates over batches in the `train_loader`. For each batch, it zeroes the gradients, performs a forward pass to compute the loss, accumulates the loss, performs backpropagation, and updates the model parameters using the optimizer. The average loss for the epoch is returned. The training loop runs for three epochs, calling the `train` function for each epoch and printing the average loss."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T14:00:03.454417Z","iopub.status.busy":"2024-10-01T14:00:03.453131Z","iopub.status.idle":"2024-10-01T15:36:08.472387Z","shell.execute_reply":"2024-10-01T15:36:08.469635Z","shell.execute_reply.started":"2024-10-01T14:00:03.454361Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","c:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/3, Loss: 0.7021\n","Epoch 2/3, Loss: 0.6771\n","Epoch 3/3, Loss: 0.5825\n"]}],"source":["from transformers import BertForSequenceClassification, AdamW\n","import torch\n","\n","# Load pre-trained BERT model\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# Set up the optimizer\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Determine the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Training function with gradient accumulation\n","def train(model, train_loader, optimizer, accumulation_steps=4):\n","    model.train()\n","    total_loss = 0\n","    optimizer.zero_grad()\n","    for i, batch in enumerate(train_loader):\n","        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","        loss.backward()\n","\n","        if (i + 1) % accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","    return total_loss / len(train_loader)\n","\n","# Training loop\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    avg_loss = train(model, train_loader, optimizer)\n","    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')"]},{"cell_type":"markdown","metadata":{},"source":["The code defines a function `get_predictions` to obtain predictions from a model using a data loader. The function sets the model to evaluation mode with `model.eval()` and initializes an empty list for predictions. It uses `torch.no_grad()` to disable gradient calculation, iterating over batches in the `data_loader`. For each batch, it extracts `input_ids` and `attention_mask`, performs a forward pass through the model, and computes the predicted class labels using `torch.max` on the model's logits. The predictions are converted to a NumPy array and appended to the predictions list. Finally, the function returns the list of predictions. The function is then called to get predictions on the test set, storing the results in `test_predictions`."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T15:36:08.479294Z","iopub.status.busy":"2024-10-01T15:36:08.476999Z","iopub.status.idle":"2024-10-01T15:38:29.068350Z","shell.execute_reply":"2024-10-01T15:38:29.066954Z","shell.execute_reply.started":"2024-10-01T15:36:08.479218Z"},"trusted":true},"outputs":[],"source":["# Get Predictions\n","\n","# Function to get predictions\n","def get_predictions(model, data_loader):\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids, attention_mask, _ = batch\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            _, pred = torch.max(outputs.logits, dim=1)\n","            predictions.extend(pred.cpu().numpy())\n","    return predictions\n","\n","# Get predictions on test set\n","test_predictions = get_predictions(model, test_loader)\n","\n","# save predictions to results folder names bert.csv\n","pd.DataFrame(test_predictions).to_csv('results/bert.csv', index=False)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T15:38:29.073774Z","iopub.status.busy":"2024-10-01T15:38:29.073181Z","iopub.status.idle":"2024-10-01T15:40:49.326100Z","shell.execute_reply":"2024-10-01T15:40:49.324924Z","shell.execute_reply.started":"2024-10-01T15:38:29.073717Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.7446\n"]}],"source":["def evaluate(model, test_loader):\n","    model.eval()\n","    total_correct = 0\n","    total_examples = 0\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids, attention_mask, labels = batch\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            predictions = torch.argmax(outputs.logits, dim=-1)\n","            total_correct += (predictions == labels).sum().item()\n","            total_examples += labels.size(0)\n","    accuracy = total_correct / total_examples\n","    return accuracy\n","\n","# Evaluate the model\n","accuracy = evaluate(model, test_loader)\n","print(f'Test Accuracy: {accuracy:.4f}')\n"]},{"cell_type":"markdown","metadata":{},"source":["The code imports `confusion_matrix` from `sklearn.metrics` and `numpy` as `np`. It defines a function `evaluate_confusion_matrix` to create a confusion matrix for evaluating the model. The function sets the model to evaluation mode with `model.eval()` and initializes empty lists for predictions and actual labels. Using `torch.no_grad()` to disable gradient calculation, it iterates over batches in the `test_loader`. For each batch, it extracts `input_ids`, `attention_mask`, and `labels`, performs a forward pass through the model, and appends the predicted class labels (obtained using `torch.argmax` on the model's logits) and actual labels to their respective lists. The function returns a confusion matrix computed from the actual and predicted labels using `confusion_matrix(actuals, predictions)`. The confusion matrix is then evaluated and printed by calling `evaluate_confusion_matrix(model, test_loader)` and storing the result in `conf_matrix`."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-01T15:40:49.328290Z","iopub.status.busy":"2024-10-01T15:40:49.327788Z","iopub.status.idle":"2024-10-01T15:43:11.077558Z","shell.execute_reply":"2024-10-01T15:43:11.076105Z","shell.execute_reply.started":"2024-10-01T15:40:49.328239Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[63 12]\n"," [35 74]]\n"]}],"source":["# create a confusion matrix to evaluate the model\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","\n","def evaluate_confusion_matrix(model, test_loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids, attention_mask, labels = batch\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            predictions.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy())\n","            actuals.extend(labels.cpu().numpy())\n","    return confusion_matrix(actuals, predictions)\n","\n","conf_matrix = evaluate_confusion_matrix(model, test_loader)\n","print(conf_matrix)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5434026,"sourceId":9017905,"sourceType":"datasetVersion"},{"datasetId":5799509,"sourceId":9524338,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":4}
