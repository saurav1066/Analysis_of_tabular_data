{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train_size  simulation  rf_accuracy\n",
      "0          50           0     0.877778\n",
      "1         100           0     0.883333\n",
      "2         150           0     0.872222\n",
      "3         200           0     0.905556\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "data_train = pd.read_csv('../data/heart_disease/data_train.csv')\n",
    "data_test = pd.read_csv('../data/heart_disease/data_test.csv')\n",
    "\n",
    "# Simulation parameters\n",
    "sample_sizes = np.linspace(50, 200, 4, dtype=int)  # Training sizes from 50 to 200 in 4 steps\n",
    "num_tests = 180  # Number of test instances\n",
    "num_simulations = 1  # Perform 1 simulation for each size\n",
    "\n",
    "# Initialize Random Forest model\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "# Function to train and predict\n",
    "def train_and_predict(model, X_train, y_train, X_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "# Main simulation loop\n",
    "results = []\n",
    "for size in sample_sizes:\n",
    "    for sim in range(num_simulations):  # This now runs only once per sample size\n",
    "        # Randomly sample training data\n",
    "        X_train_sample, _, y_train_sample, _ = train_test_split(\n",
    "            data_train.drop('num', axis=1), data_train['num'], train_size=size, random_state=sim)\n",
    "        \n",
    "        # Select a fixed number of test instances\n",
    "        X_test_sample = data_test.drop('num', axis=1).sample(n=num_tests, random_state=sim)\n",
    "        y_test_sample = data_test.loc[X_test_sample.index, 'num']\n",
    "        \n",
    "        # Train and predict with Random Forest\n",
    "        rf_predictions = train_and_predict(random_forest, X_train_sample, y_train_sample, X_test_sample)\n",
    "        \n",
    "        # Evaluate predictions using accuracy\n",
    "        rf_accuracy = accuracy_score(y_test_sample, rf_predictions)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'train_size': size,\n",
    "            'simulation': sim,\n",
    "            'rf_accuracy': rf_accuracy\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize each row individually\n",
    "def tokenize_row(row, tokenizer):\n",
    "    # Convert the row to string, as BERT expects textual input\n",
    "    row_str = \" \".join(map(str, row.values))\n",
    "    inputs = tokenizer(row_str, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "# Tokenize the training and testing data\n",
    "train_inputs = [tokenize_row(row, tokenizer) for _, row in data_train.iterrows()]\n",
    "test_inputs = [tokenize_row(row, tokenizer) for _, row in data_test.iterrows()]\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HeartDiseaseDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs[idx]['input_ids'].squeeze(0)  # Remove batch dimension\n",
    "        attention_mask = self.inputs[idx]['attention_mask'].squeeze(0)\n",
    "        label = torch.tensor(self.labels.iloc[idx])\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = HeartDiseaseDataset(train_inputs, y_train)\n",
    "test_dataset = HeartDiseaseDataset(test_inputs, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_data(data, tokenizer):\n",
    "    return [tokenize_row(row, tokenizer) for _, row in data.iterrows()]\n",
    "\n",
    "def tokenize_row(row, tokenizer):\n",
    "    row_str = \" \".join(map(str, row.values))\n",
    "    inputs = tokenizer(row_str, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "# Dataset class\n",
    "class HeartDiseaseDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs[idx]['input_ids'].squeeze(0)  # Remove batch dimension\n",
    "        attention_mask = self.inputs[idx]['attention_mask'].squeeze(0)\n",
    "        label = torch.tensor(self.labels.iloc[idx])\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "# Training function with gradient accumulation\n",
    "def train(model, train_loader, optimizer, device, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Prepare tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Simulation parameters\n",
    "sample_sizes = np.linspace(50, 200, 4, dtype=int)\n",
    "num_simulations = 4\n",
    "\n",
    "# Initialize BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "results = []\n",
    "for size in sample_sizes:\n",
    "    for sim in range(num_simulations):\n",
    "        # Sample and tokenize training data\n",
    "        train_data, _ = train_test_split(data_train, train_size=size, random_state=sim)\n",
    "        train_inputs = tokenize_data(train_data, tokenizer)\n",
    "        train_labels = train_data['label']  # Assume label column is named 'label'\n",
    "\n",
    "        # Create train loader\n",
    "        train_dataset = HeartDiseaseDataset(train_inputs, train_labels)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "        # Training BERT\n",
    "        avg_loss = train(model, train_loader, optimizer, device)\n",
    "        \n",
    "        # Simulate test set predictions, assuming you need predictions for a fixed number of test instances\n",
    "        test_inputs = tokenize_data(data_test.sample(30, random_state=sim), tokenizer)\n",
    "        test_labels = data_test['label'].iloc[:30]  # Modify as per your test sampling logic\n",
    "        test_dataset = HeartDiseaseDataset(test_inputs, test_labels)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "        test_predictions = get_predictions(model, test_loader)\n",
    "\n",
    "        # Evaluate the predictions as required (e.g., accuracy)\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'train_size': size,\n",
    "            'simulation': sim,\n",
    "            'loss': avg_loss,\n",
    "            'predictions': test_predictions  # Or any other metric\n",
    "        })\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
